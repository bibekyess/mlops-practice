It seems tracking the training script on a MLflow server running on kubernetes is not a viable option. It is cubersome to change the parameter, build image, load it and then rerun the script.
